# AI Gaming Agent ðŸ¤– + ðŸŽ®

The project aims to apply a comprehensive AI gaming agent framework built on top of **smolagents** to create intelligent agents capable of playing and mastering various types of games, from simple board games to complex universal gaming environments.

## Technology Stack

- [SmolAgents](https://github.com/huggingface/smolagents): A lightweight framework for building AI agents.
- [Cradle](https://github.com/BAAI-Agents/Cradle): A framework which attempts at General Computer Control (GCC).

## Environment Installation

It is suggested to use a remote virtual environment for environment configurations to prevent from directly executing the Python code generated by the AI agent locally. For details please take reference to [SmolAgent Documentation](https://huggingface.co/docs/smolagents/index).

1. Please install a virtual environment.

   ```
   python3 -m venv .venv # Name it as .venv
   source .venv/bin/activate # Activate .venv
   ```

2. Install necessary libraries.

   ```
   pip install -r requirements.txt
   ```

3. **Install GroundingDINO** for precise visual detection:

   **Quick Setup (Recommended):**

   Linux/macOS:

   ```bash
   ./setup_groundingdino.sh
   ```

   Windows:

   ```cmd
   setup_groundingdino.bat
   ```

   **Manual Setup:**

   ```bash
   # Install GroundingDINO
   pip install groundingdino-py

   # Download model files to ./cache/
   mkdir -p cache
   cd cache
   wget https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinB.cfg.py -O GroundingDINO_SwinB_cfg.py
   wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth
   cd ..
   ```

   **Note**: The agent works without GroundingDINO (vision-only mode), but GroundingDINO provides much better accuracy for clicking visual elements like icons, buttons, and game objects.

4. Create a `.env` file for storing api key

   ```
   echo 'OPENAI_API_KEY=your-api-key' > .env
   ```

   Or export as environment variable:

   ```
   export OPENAI_API_KEY='your-api-key'
   ```

5. Running Tests
   Without LLM (basic module tests):

   ```
   python -m src.modules.test.test_without_llm
   ```

   With LLM (requires API key):

   ```
   python -m src.modules.test.test_modules
   ```

   Check the window capture:

   ```
   python -m src.modules.test.test_window_capture [optional_window_title]
   ```

6. Usage

   **Quick Start (Recommended):**

   Linux/macOS:

   ```bash
   ./run_agent.sh
   ```

   Windows:

   ```cmd
   run_agent.bat
   ```

   Interactive launcher with two modes:

   - **Fullscreen**: Capture entire monitor (choose monitor 0/1/2/...)
   - **Specific Window**: Capture only target window (higher resolution, better accuracy)

   **Manual Usage:**

   Fullscreen on primary monitor:

   Linux/macOS:

   ```bash
   MONITOR_INDEX=1 python -m src.modules.main "Your task here"
   ```

   Windows (PowerShell):

   ```powershell
   $env:MONITOR_INDEX=1; python -m src.modules.main "Your task here"
   ```

   Windows (CMD):

   ```cmd
   set MONITOR_INDEX=1 && python -m src.modules.main "Your task here"
   ```

   Specific window:

   Linux/macOS:

   ```bash
   WINDOW_TITLE="Microsoft Edge" python -m src.modules.main "Your task here"
   ```

   Windows (PowerShell):

   ```powershell
   $env:WINDOW_TITLE="Microsoft Edge"; python -m src.modules.main "Your task here"
   ```

   Windows (CMD):

   ```cmd
   set WINDOW_TITLE=Microsoft Edge && python -m src.modules.main "Your task here"
   ```

   **Windows Requirements:**

   - Window capture requires `pywin32`: `pip install pywin32`
   - Falls back to fullscreen if not installed

   Notes:

   - Press `b` during prompts to go back (interactive mode)
   - `MONITOR_INDEX`: 0=all, 1=primary, 2=secondary, etc.
   - Window mode automatically detects which monitor contains the window

7. View logs and tokens

   All terminal output is automatically saved to timestamped log files. Logs include full prompts, responses, and token usage for all LLM calls.

   ```
   src/modules/memory/task_log/task_YYYYMMDD_HHMMSS.log
   ```

   Check for tokens used. Remember to change the log name in line 143 in code.

   ```
   python src/modules/memory/task_log/analyze_tokens.py
   ```

8. Reset memory

   Linux/macOS:

   ```bash
   rm src/modules/memory/data/*.json
   rm src/modules/memory/data/*.npy
   rm src/modules/screen_input/screenshots/*.jpg
   rm src/modules/memory/task_log/*.log
   ```

   Windows (PowerShell):

   ```powershell
   Remove-Item src/modules/memory/data/*.json
   Remove-Item src/modules/memory/data/*.npy
   Remove-Item src/modules/screen_input/screenshots/*.jpg
   Remove-Item src/modules/memory/task_log/*.log
   ```

   Windows (CMD):

   ```cmd
   del src\modules\memory\data\*.json
   del src\modules\memory\data\*.npy
   del src\modules\screen_input\screenshots\*.jpg
   del src\modules\memory\task_log\*.log
   ```

## Agent Architecture

The architecture is reference from [Cradle](https://github.com/BAAI-Agents/Cradle). Please read [ARCHITECTURE.md](ARCHITECTURE.md) for details.

```mermaid
---
config:
  layout: dagre
---
flowchart TB
 subgraph s1["**Data Input**"]
        A["Screen Input"]
        B["Information<br>Gathering"]
  end
 subgraph s2["**Agent Thinking**"]
        D["Self-Reflection"]
        E["Task Inference"]
        F["Skill Curation"]
        G["Action Planning"]
  end
 subgraph s3["**UI Execution**"]
        H["UI Automation"]
        J["Computer Environment"]
  end
 subgraph s4["**Memory System**"]
        C["Memory"]
  end
    A --> B
    B --> C
    C --> D & E
    D --> C
    E --> F
    F --> G
    G --> H
    H --> J
    J --> A
    style s1 fill:#FFE0B2
    style s2 fill:#C8E6C9
    style s3 fill:#FFCDD2
    style s4 fill:#E1BEE7

```

## Code Structure

```mermaid
---
config:
  layout: dagre
---
flowchart TB
    subgraph Input["**Data Input**"]
        A[Screen Capture<br/>screen_capture.py]
        B[Info Gathering<br/>info_gather.py<br/>Vision LLM + OCR]
        B2[Object Detection<br/>object_detector.py<br/>GroundingDINO]
    end

    subgraph Memory["**Memory System**"]
        C1[Short-term Memory<br/>short_term.py<br/>short_term_state.json]
        C2[Long-term Skills<br/>long_term.py<br/>skills.json]
        C3[Skill Retrieval<br/>skill_retrieval.py<br/>skill_embeddings.npy]
    end

    subgraph Reasoning["**Agent Thinking**"]
        D[Self-Reflection<br/>reflector.py<br/>LLM judges success/failure]
        E[Task Inference<br/>task_breaker.py<br/>Decompose to subtasks]
        F[Skill Curation<br/>skill_manager.py<br/>Extract and save skills]
        G[Action Planning<br/>planner.py<br/>CodeAgent + Tools + Skills]
    end

    subgraph Output["**UI Execution**"]
        H1[Atomic Actions<br/>atomic_actions.py<br/>Low-level primitives]
        H2[Action Executor<br/>executor.py<br/>Executes planned actions]
        H3[Tools<br/>tools.py<br/>Validated @tool wrappers]
        I[Environment]
    end

    A -->|screenshot| B
    A -->|screenshot| B2
    B2 -->|detected objects| B
    B -->|observation| C1

    C1 -->|state| D
    C1 -->|task| E

    E -->|subtasks| C1
    E -->|current subtask| G

    G -->|query subtask| C3
    C3 -->|vector search| C2
    C2 -->|top-3 skill codes| G

    D -->|before/after obs| B
    D -->|if failed| E
    D -->|if success pattern| F

    F -->|save new skill| C2
    F -->|update vectors| C3

    G -->|action dict| H2
    H2 -->|execute action| H3
    H3 -->|calls| H1
    H1 -->|execute| I
    I -->|screen changes| A

    style Input fill:#FFE0B2
    style Memory fill:#B3E5FC
    style Reasoning fill:#C8E6C9
    style Output fill:#F8BBD0
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- **Hugging Face** for the amazing smolagents framework
