# AI Gaming Agent ðŸ¤– + ðŸŽ®

The project aims to apply a comprehensive AI gaming agent framework built on top of **smolagents** to create intelligent agents capable of playing and mastering various types of games, from simple board games to complex universal gaming environments.

## Technology Stack

- [SmolAgents](https://github.com/huggingface/smolagents): A lightweight framework for building AI agents.
- [Cradle](https://github.com/BAAI-Agents/Cradle): A framework which attempts at General Computer Control (GCC).

## Environment Installation

It is suggested to use a remote virtual environment for environment configurations to prevent from directly executing the Python code generated by the AI agent locally. For details please take reference to [SmolAgent Documentation](https://huggingface.co/docs/smolagents/index).

1. Please install a virtual environment.

   ```
   python3 -m venv .venv # Name it as .venv
   source .venv/bin/activate # Activate .venv
   ```

2. Install necessary libraries.

   ```
   pip install -r requirements.txt
   ```

3. **(Optional) Install GroundingDINO** for precise visual detection:

   **Quick Setup (Recommended):**

   ```bash
   ./setup_groundingdino.sh
   ```

   **Manual Setup:**

   ```bash
   # Install GroundingDINO
   pip install groundingdino-py

   # Download model files to ./cache/
   mkdir -p cache
   cd cache
   wget https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinB.cfg.py -O GroundingDINO_SwinB_cfg.py
   wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth
   cd ..
   ```

   **Note**: The agent works without GroundingDINO (vision-only mode), but GroundingDINO provides much better accuracy for clicking visual elements like icons, buttons, and game objects.

4. Create a `.env` file for storing api key

   ```
   echo 'OPENAI_API_KEY=your-api-key' > .env
   ```

   Or export as environment variable:

   ```
   export OPENAI_API_KEY='your-api-key'
   ```

5. Running Tests
   Without LLM (basic module tests):

   ```
   python -m src.modules.test.test_without_llm
   ```

   With LLM (requires API key):

   ```
   python -m src.modules.test.test_modules
   ```

   Check the window capture:
   ```
   python -m src.modules.test.test_window_capture [optional_window_title]
   ```

6. Usage

   Full screen
   ```
   python -m src.modules.main
   ```

   Choose the specific window (matching with the most similar name of window)

   ```
   $env:WINDOW_TITLE = "chrome"
   python -m src.modules.main
   ```

   Notes:
   - You can also set `WINDOW_TITLE` in your .env file.

   Or with a custom task:

   ```
   python -m src.modules.main "Open Chrome and search for OpenAI"
   ```

7. View logs

   All terminal output is automatically saved to timestamped log files:

   ```
   src/modules/memory/task_log/task_YYYYMMDD_HHMMSS.log
   ```

   Logs include full prompts, responses, and token usage for all LLM calls.

8. Reset memory
   ```
   rm src/modules/memory/data/*.json
   rm src/modules/memory/data/*.npy
   rm src/modules/screen_input/screenshots/*.jpg
   rm src/modules/memory/task_log/*.log
   ```

## Agent Architecture

The architecture is reference from [Cradle](https://github.com/BAAI-Agents/Cradle). Please read [ARCHITECTURE.md](ARCHITECTURE.md) for details.

```mermaid
---
config:
  layout: dagre
---
flowchart TB
 subgraph s1["**Data Input**"]
        A["Screen Input"]
        B["Information<br>Gathering"]
  end
 subgraph s2["**Agent Thinking**"]
        D["Self-Reflection"]
        E["Task Inference"]
        F["Skill Curation"]
        G["Action Planning"]
  end
 subgraph s3["**UI Execution**"]
        H["UI Automation"]
        J["Computer Environment"]
  end
 subgraph s4["**Memory System**"]
        C["Memory"]
  end
    A --> B
    B --> C
    C --> D & E
    D --> C
    E --> F
    F --> G
    G --> H
    H --> J
    J --> A
    style s1 fill:#FFE0B2
    style s2 fill:#C8E6C9
    style s3 fill:#FFCDD2
    style s4 fill:#E1BEE7

```

## Code Structure

```mermaid
---
config:
  layout: dagre
---
flowchart TB
    subgraph Input["**Data Input**"]
        A[Screen Capture<br/>screen_capture.py]
        B[Info Gathering<br/>info_gather.py<br/>Vision LLM + OCR]
        B2[Object Detection<br/>object_detector.py<br/>GroundingDINO]
    end

    subgraph Memory["**Memory System**"]
        C1[Short-term Memory<br/>short_term.py<br/>short_term_state.json]
        C2[Long-term Skills<br/>long_term.py<br/>skills.json]
        C3[Skill Retrieval<br/>skill_retrieval.py<br/>skill_embeddings.npy]
    end

    subgraph Reasoning["**Agent Thinking**"]
        D[Self-Reflection<br/>reflector.py<br/>LLM judges success/failure]
        E[Task Inference<br/>task_breaker.py<br/>Decompose to subtasks]
        F[Skill Curation<br/>skill_manager.py<br/>Extract and save skills]
        G[Action Planning<br/>planner.py<br/>CodeAgent + Tools + Skills]
    end

    subgraph Output["**UI Execution**"]
        H1[Atomic Actions<br/>atomic_actions.py<br/>Low-level primitives]
        H2[Action Executor<br/>executor.py<br/>Executes planned actions]
        H3[Tools<br/>tools.py<br/>Validated @tool wrappers]
        I[Environment]
    end

    A -->|screenshot| B
    A -->|screenshot| B2
    B2 -->|detected objects| B
    B -->|observation| C1

    C1 -->|state| D
    C1 -->|task| E

    E -->|subtasks| C1
    E -->|current subtask| G

    G -->|query subtask| C3
    C3 -->|vector search| C2
    C2 -->|top-3 skill codes| G

    D -->|before/after obs| B
    D -->|if failed| E
    D -->|if success pattern| F

    F -->|save new skill| C2
    F -->|update vectors| C3

    G -->|action dict| H2
    H2 -->|execute action| H3
    H3 -->|calls| H1
    H1 -->|execute| I
    I -->|screen changes| A

    style Input fill:#FFE0B2
    style Memory fill:#B3E5FC
    style Reasoning fill:#C8E6C9
    style Output fill:#F8BBD0
```

## Log

### 23/10/2025

- Unified window capture across Windows/macOS/Linux in a single API:
   - Windows: Win32 PrintWindow for true background window capture; automatic fullscreen fallback if unavailable
   - macOS: Quartz CGWindowListCreateImage for a specific window; automatic fullscreen fallback if unavailable
   - Linux: Prefer `xwd` for window dump; fallback to `xwininfo` geometry + mss; automatic fullscreen fallback when tools are missing
- Added fuzzy window title matching (substring first, then fuzzy) and window list printing for easier selection
- Added a new test case `src/modules/test/test_window_capture.py`
- Agent now supports prioritized window capture when a title is configured via environment variable:
   - `WINDOW_TITLE` (e.g., `$env:WINDOW_TITLE = "chrome"` on PowerShell)
   - If not set, behavior remains fullscreen-only
- Requirements updated for macOS:
   - Added `pyobjc-framework-Quartz; sys_platform == "darwin"` for macOS window capture
- Linux system tools (optional, not in pip requirements): `wmctrl`, `xdotool`, `xwininfo`, `xwd`
- Test of window capturing only done on WindowsOS

### 22/10/2025

- Added comprehensive LLM logging: all prompts, responses, and token usage are now logged
- Added automatic log file saving to `src/modules/memory/task_log/` with timestamps
- All terminal output is captured and saved for debugging and analysis
- Added TaskLogger class with TeeOutput for simultaneous console and file logging
- **Hybrid Vision + Object Detection System**: Vision model can now request object detection for precise coordinates
  - Vision LLM handles strategic decisions and context understanding
  - GroundingDINO provides precise visual element localization when requested
  - Automatic fallback: works without GroundingDINO (vision-only mode)
  - Vision model outputs `REQUEST_DETECTION: <object>` for visual elements without text
  - System annotates images with bounding boxes and re-analyzes for precise coordinates
- **Removed YOLO**: Simplified to GroundingDINO only (better for games, zero-shot detection)
- **Performance optimizations**: Image resizing (1024px max), simplified prompts, removed grid system

### 15/10/2025

- Focus on image contextual memory (image to text description)
- Memory store screenshots (older screenshots more blur, and store limited number of screenshots, buffer zone)
- Screenshots frequency
- Memory limit, delete old memory?
- Test if the saved succesfull memory skills can be reused
- Compare strong model without vision support and weak model with vison support (ocr, yolo...)
- Store reflection into JSON everytime?
- Screenshot specific window?

### 8/10/2025

- Add EasyOCR for character detection
- Add YOLO for object detection (later replaced with GroundingDINO)
- Adjust the screen capture to native resolution capture instead of logical resolution capture

### 5/10/2025

- Added action_planning module
- Added self_reflection module
- Added skill_curation module
- Added task_inference module
- Added test module
- Performance not optimal, should optimize it
  - llm cannot compare contextual semantic screenshots, that is, it can not directly notice the change in screenshots
  - llm can struggles on one page for a long time
  - does the atomic actions work on multiple operating systems (MacOS, Windows, Linux)?

### 2/10/2025

- Added ui_automation module
- Added screen_input module
- Added information_gathering module
- Added memory module

### 27/9/2025

- Created modules folder
- Designed developing process

### 23/9/2025

- Added demo and reference from ByteDance Tars

### 18/9/2025

- Initialized project

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- **Hugging Face** for the amazing smolagents framework